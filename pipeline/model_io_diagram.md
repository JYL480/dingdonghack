# Model I/O Diagram - Android Automation Pipeline

## ğŸ¯ Current System Overview

An intelligent Android automation pipeline that uses vision-capable AI models to understand screens and execute complex multi-step tasks through a Supervisor-Worker architecture.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     ORCHESTRATOR                             â”‚
â”‚                  (orchestrator.py)                           â”‚
â”‚                                                              â”‚
â”‚  â€¢ Main execution loop                                       â”‚
â”‚  â€¢ Coordinates Supervisor â†” Worker communication             â”‚
â”‚  â€¢ Handles timeouts and max steps                           â”‚
â”‚  â€¢ Creates task folders for screenshots                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                           â”‚
             â–¼                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    SUPERVISOR V2 ğŸ‘ï¸      â”‚  â”‚       WORKER ğŸ¤–             â”‚
â”‚ (supervisor_agent_v2.py) â”‚  â”‚    (worker_agent.py)         â”‚
â”‚                          â”‚  â”‚                              â”‚
â”‚ WITH VISION:             â”‚  â”‚ EXECUTES:                    â”‚
â”‚ â€¢ Sees screenshots       â”‚  â”‚ â€¢ Two-stage navigation       â”‚
â”‚ â€¢ Creates subtasks       â”‚  â”‚ â€¢ Stage 1: Vision (what)     â”‚
â”‚ â€¢ Validates every action â”‚  â”‚ â€¢ Stage 2: Action (do it)    â”‚
â”‚ â€¢ Provides corrections   â”‚  â”‚ â€¢ Takes screenshots          â”‚
â”‚ â€¢ Detects stuck states   â”‚  â”‚ â€¢ Executes ADB commands      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                           â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                    [gpt-4o-mini]
                    Vision Model
                         â”‚
                    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
                    â”‚ ANDROID â”‚
                    â”‚ DEVICE  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“Š Enhanced Data Flow

### 1. Task Decomposition (Full Upfront)
```
User: "Open playstore and install tiktok"
                    â†“
Supervisor: [LLM decomposes entire task]
   â†’ Subtask 1: Open Play Store app
   â†’ Subtask 2: Tap search bar
   â†’ Subtask 3: Type "tiktok"
   â†’ Subtask 4: Tap search button
   â†’ Subtask 5: Select TikTok from results
   â†’ Subtask 6: Tap Install button
```

### 2. Execution Loop
```
For each subtask:
   Worker executes â†’ Screenshot â†’ Supervisor validates
      â†“                              â†“
   If correct: âœ“ Next              If wrong: Correction
```

### 3. Coordinate Extrapolation (New!)
```
Worker Vision: "Search bar has no element number"
               "Element 5 (Google logo) at (352, 234)"
               "Search bar is ~200 pixels below"
                    â†“
Worker Decision: "tap 200 pixels below element 5"
                    â†“
System: Calculates (352, 434) and taps there
```

### 4. Validation (Every Action)
```
Worker: "Tapped coordinate (352, 434)"
Supervisor: [Validates with screenshot]
   â†’ âœ“ "Search bar is now focused" OR
   â†’ âœ— "Wrong location, try 250 pixels below"
```

## ğŸ”§ Key Components & Changes

### Orchestrator (`orchestrator/orchestrator.py`)
**No changes** - Still the main coordinator
- Manages execution flow
- Tracks progress (1/3 subtasks complete, etc.)
- Handles timeouts (5 min default)

### Supervisor V2 (`supervisor/supervisor_agent_v2.py`)
**Key Features:**
- âœ… Full task decomposition via LLM (breaks complex tasks into atomic steps)
- âœ… Vision-based validation of every worker action
- âœ… Dynamic task refinement based on screen state
- âœ… Intelligent stuck detection (max 3 retries then skip)
- âœ… Context-aware corrections without hardcoding
- âœ… Tracks original task throughout execution
- âœ… Can mark subtasks as failed and continue

**Key Methods:**
```python
decompose_task(task, screenshot)  # Creates subtasks with specific values
verify_subtask_completion(subtask, screenshot)  # Validates with vision
analyze_stuck_situation(feedback, screenshot)  # Understands why stuck
_is_stuck()  # Detects retry loops (enhanced)
```

### Worker (`worker/worker_agent.py`)
**Key Features:**
- âœ… Two-stage navigation (Vision â†’ Action)
- âœ… Coordinate extrapolation for unmarked elements
- âœ… Clears text fields before typing
- âœ… Uses actual element coordinates for estimation
- âœ… Types full words/phrases at once
- âœ… Screen-aware distance calculations

**Key Flow:**
```python
# Gets subtask with specific_value: "81291126"
# Types actual value, not "your_email@example.com"
```

### Two-Stage Navigator (`utils/nich_utils_twostage.py`)
**Core Capabilities:**
- âœ… Enhanced vision prompts with element coordinates
- âœ… Coordinate inference for unmarked UI elements
- âœ… Smart decision parsing (preserves coordinate instructions)
- âœ… Screen dimension-based distance estimation
- âœ… Supports: tap between elements, pixel offsets, screen positions
- âœ… No hardcoded patterns or examples

## ğŸš€ What We Have Now

### Strengths
1. **AI-Driven Understanding** - Supervisor sees and understands screens
2. **Specific Value Handling** - Types "81291126" not generic examples
3. **Robust Stuck Detection** - Stops after 3 retries, marks as failed
4. **Immediate Validation** - Every action is checked visually
5. **Simple Architecture** - No duplicate screen detection logic

### How It Works
```python
# Example Flow
Task: "Type 81291126 in the field"
â†’ Supervisor extracts: specific_value = "81291126"
â†’ Worker receives: subtask with specific_value
â†’ Worker types: "81291126" (not example text)
â†’ Supervisor validates: Checks if typed correctly
â†’ If error: Supervisor detects stuck after 3 tries
â†’ Marks failed and moves to next subtask
```

### Error Handling
```python
# Stuck Detection
if typing same value 3 times:
    mark subtask as "failed"
    move to next subtask
    log: "Unable to complete after 3 attempts"

# Screen Transition Delays
time.sleep(3)  # Wait for screens to fully load before validation

# Field Clearing
clear_text()  # Clear existing text before typing new values
```

## ğŸ¯ Design Philosophy

**Before:** Try to predict what screen we're on with patterns
**Now:** Let the AI see and understand

**Before:** Complex screen detection with hardcoded apps
**Now:** Supervisor looks at screenshot and knows

**Before:** Infinite retry loops
**Now:** Max 3 retries then skip

**Before:** Generic example values
**Now:** Actual requested values

## ğŸ“ File Structure (Current)
```
pipeline/
â”œâ”€â”€ orchestrator/
â”‚   â”œâ”€â”€ orchestrator.py         # Main loop (unchanged)
â”‚   â””â”€â”€ orchestrator_async.py   # Async version
â”œâ”€â”€ supervisor/
â”‚   â””â”€â”€ supervisor_agent_v2.py  # Vision supervisor (enhanced)
â”œâ”€â”€ worker/
â”‚   â””â”€â”€ worker_agent.py         # Task executor (enhanced)
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ nich_utils.py           # Core utilities
â”‚   â””â”€â”€ nich_utils_twostage.py  # Two-stage nav (simplified)
â””â”€â”€ nich_pipeline.py            # Entry point
```

## ğŸ”„ Recent Improvements

1. **AI-Driven Validation**
   - No hardcoded screen transitions
   - Vision model understands context
   - Recognizes implicit task completion

2. **Screen Transition Handling**
   - Added delays for screen loading
   - Fresh screenshots for validation
   - Prevents wrong-field typing

3. **Field Management**
   - Clear text before typing
   - Prevents duplicate text
   - Proper value validation

4. **Validation on Every Action**
   - Not just when stuck
   - Immediate correction
   - Better success rate

## ğŸ’¡ Key Insight

**The supervisor with vision doesn't need our help to understand screens.**

We were duplicating effort - trying to detect screens with patterns when the AI can just look and understand. Now the system is simpler and more robust.